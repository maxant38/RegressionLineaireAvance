{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "import statsmodels.formula.api as smf\n",
    "import xgboost as xgb\n",
    "seed = 1222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X291</th>\n",
       "      <th>X292</th>\n",
       "      <th>X293</th>\n",
       "      <th>X294</th>\n",
       "      <th>X295</th>\n",
       "      <th>X296</th>\n",
       "      <th>X297</th>\n",
       "      <th>X298</th>\n",
       "      <th>X299</th>\n",
       "      <th>X300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0313</td>\n",
       "      <td>1.8425</td>\n",
       "      <td>-2.0760</td>\n",
       "      <td>-3.3012</td>\n",
       "      <td>0.2591</td>\n",
       "      <td>-1.4561</td>\n",
       "      <td>-0.4950</td>\n",
       "      <td>0.8914</td>\n",
       "      <td>-0.2391</td>\n",
       "      <td>1.8337</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4625</td>\n",
       "      <td>-0.5273</td>\n",
       "      <td>-0.7469</td>\n",
       "      <td>0.6998</td>\n",
       "      <td>-0.4887</td>\n",
       "      <td>-0.9524</td>\n",
       "      <td>0.3629</td>\n",
       "      <td>0.7396</td>\n",
       "      <td>-0.9543</td>\n",
       "      <td>-1.3848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.7081</td>\n",
       "      <td>1.5196</td>\n",
       "      <td>-0.6949</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>3.2777</td>\n",
       "      <td>2.7047</td>\n",
       "      <td>1.3653</td>\n",
       "      <td>-2.2654</td>\n",
       "      <td>2.4361</td>\n",
       "      <td>-1.2674</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1674</td>\n",
       "      <td>-3.0499</td>\n",
       "      <td>0.4132</td>\n",
       "      <td>1.6582</td>\n",
       "      <td>1.8960</td>\n",
       "      <td>3.9402</td>\n",
       "      <td>0.2812</td>\n",
       "      <td>0.2473</td>\n",
       "      <td>-1.6407</td>\n",
       "      <td>0.0612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.9505</td>\n",
       "      <td>-0.4392</td>\n",
       "      <td>3.6427</td>\n",
       "      <td>-6.3903</td>\n",
       "      <td>0.4406</td>\n",
       "      <td>-0.9868</td>\n",
       "      <td>0.4398</td>\n",
       "      <td>5.2369</td>\n",
       "      <td>-0.6638</td>\n",
       "      <td>0.8670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3530</td>\n",
       "      <td>-0.8737</td>\n",
       "      <td>-1.1940</td>\n",
       "      <td>1.9316</td>\n",
       "      <td>-6.0107</td>\n",
       "      <td>0.3642</td>\n",
       "      <td>-1.5129</td>\n",
       "      <td>-4.1881</td>\n",
       "      <td>3.0166</td>\n",
       "      <td>1.7286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.7782</td>\n",
       "      <td>-0.6504</td>\n",
       "      <td>-1.0025</td>\n",
       "      <td>2.7306</td>\n",
       "      <td>1.9346</td>\n",
       "      <td>0.2506</td>\n",
       "      <td>-2.2139</td>\n",
       "      <td>-0.8253</td>\n",
       "      <td>-2.0756</td>\n",
       "      <td>2.1793</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4704</td>\n",
       "      <td>2.4512</td>\n",
       "      <td>-0.8304</td>\n",
       "      <td>0.0790</td>\n",
       "      <td>0.9600</td>\n",
       "      <td>0.5918</td>\n",
       "      <td>-0.9744</td>\n",
       "      <td>-1.3672</td>\n",
       "      <td>-1.0806</td>\n",
       "      <td>-0.5822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.6567</td>\n",
       "      <td>5.2272</td>\n",
       "      <td>-2.7754</td>\n",
       "      <td>1.0493</td>\n",
       "      <td>1.6010</td>\n",
       "      <td>2.0521</td>\n",
       "      <td>2.1493</td>\n",
       "      <td>-2.7490</td>\n",
       "      <td>0.8005</td>\n",
       "      <td>1.2301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>-3.4169</td>\n",
       "      <td>0.8112</td>\n",
       "      <td>-2.7943</td>\n",
       "      <td>3.8443</td>\n",
       "      <td>0.1666</td>\n",
       "      <td>-0.5501</td>\n",
       "      <td>1.6320</td>\n",
       "      <td>1.0897</td>\n",
       "      <td>-0.3964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.0274</td>\n",
       "      <td>-0.6006</td>\n",
       "      <td>-0.9335</td>\n",
       "      <td>1.8062</td>\n",
       "      <td>2.0059</td>\n",
       "      <td>1.1625</td>\n",
       "      <td>0.2329</td>\n",
       "      <td>0.1705</td>\n",
       "      <td>-0.8247</td>\n",
       "      <td>-2.8357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6565</td>\n",
       "      <td>-2.2997</td>\n",
       "      <td>1.7963</td>\n",
       "      <td>0.2887</td>\n",
       "      <td>-0.1655</td>\n",
       "      <td>-2.5131</td>\n",
       "      <td>1.1653</td>\n",
       "      <td>-0.9547</td>\n",
       "      <td>1.0795</td>\n",
       "      <td>1.0793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-3.2339</td>\n",
       "      <td>2.8176</td>\n",
       "      <td>-2.7580</td>\n",
       "      <td>1.3753</td>\n",
       "      <td>-3.6134</td>\n",
       "      <td>-2.6822</td>\n",
       "      <td>0.0465</td>\n",
       "      <td>-3.3970</td>\n",
       "      <td>-2.4349</td>\n",
       "      <td>-1.4289</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.8242</td>\n",
       "      <td>0.7801</td>\n",
       "      <td>0.5300</td>\n",
       "      <td>-0.2795</td>\n",
       "      <td>2.2817</td>\n",
       "      <td>0.3588</td>\n",
       "      <td>2.6076</td>\n",
       "      <td>2.0471</td>\n",
       "      <td>2.4782</td>\n",
       "      <td>-0.9987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>6.8295</td>\n",
       "      <td>-0.2040</td>\n",
       "      <td>1.1870</td>\n",
       "      <td>-1.3487</td>\n",
       "      <td>1.7526</td>\n",
       "      <td>0.6084</td>\n",
       "      <td>1.6733</td>\n",
       "      <td>1.2699</td>\n",
       "      <td>-1.3810</td>\n",
       "      <td>-1.1775</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0520</td>\n",
       "      <td>1.1615</td>\n",
       "      <td>0.7577</td>\n",
       "      <td>-2.4536</td>\n",
       "      <td>-1.6191</td>\n",
       "      <td>-1.8861</td>\n",
       "      <td>-1.3888</td>\n",
       "      <td>-1.3644</td>\n",
       "      <td>0.2562</td>\n",
       "      <td>-1.2735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0990</td>\n",
       "      <td>-1.5969</td>\n",
       "      <td>0.2615</td>\n",
       "      <td>2.8507</td>\n",
       "      <td>1.1551</td>\n",
       "      <td>-0.6740</td>\n",
       "      <td>0.9015</td>\n",
       "      <td>0.7859</td>\n",
       "      <td>0.6387</td>\n",
       "      <td>-3.1715</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1602</td>\n",
       "      <td>-4.6499</td>\n",
       "      <td>-0.0869</td>\n",
       "      <td>-1.2168</td>\n",
       "      <td>2.4656</td>\n",
       "      <td>-1.1276</td>\n",
       "      <td>-1.4000</td>\n",
       "      <td>-0.8152</td>\n",
       "      <td>-2.3054</td>\n",
       "      <td>1.1570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>8.3197</td>\n",
       "      <td>1.2937</td>\n",
       "      <td>-1.6657</td>\n",
       "      <td>-3.1165</td>\n",
       "      <td>-1.1774</td>\n",
       "      <td>-1.8673</td>\n",
       "      <td>1.0129</td>\n",
       "      <td>-1.5996</td>\n",
       "      <td>3.5014</td>\n",
       "      <td>1.1492</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.9003</td>\n",
       "      <td>-0.3482</td>\n",
       "      <td>2.0477</td>\n",
       "      <td>0.3222</td>\n",
       "      <td>1.4145</td>\n",
       "      <td>-0.7185</td>\n",
       "      <td>3.7552</td>\n",
       "      <td>-2.3238</td>\n",
       "      <td>-2.4754</td>\n",
       "      <td>-0.8485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          y      X1      X2      X3      X4      X5      X6      X7      X8  \\\n",
       "0   -1.0313  1.8425 -2.0760 -3.3012  0.2591 -1.4561 -0.4950  0.8914 -0.2391   \n",
       "1   -4.7081  1.5196 -0.6949  0.7994  3.2777  2.7047  1.3653 -2.2654  2.4361   \n",
       "2    3.9505 -0.4392  3.6427 -6.3903  0.4406 -0.9868  0.4398  5.2369 -0.6638   \n",
       "3   -2.7782 -0.6504 -1.0025  2.7306  1.9346  0.2506 -2.2139 -0.8253 -2.0756   \n",
       "4    2.6567  5.2272 -2.7754  1.0493  1.6010  2.0521  2.1493 -2.7490  0.8005   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "995  0.0274 -0.6006 -0.9335  1.8062  2.0059  1.1625  0.2329  0.1705 -0.8247   \n",
       "996 -3.2339  2.8176 -2.7580  1.3753 -3.6134 -2.6822  0.0465 -3.3970 -2.4349   \n",
       "997  6.8295 -0.2040  1.1870 -1.3487  1.7526  0.6084  1.6733  1.2699 -1.3810   \n",
       "998  0.0990 -1.5969  0.2615  2.8507  1.1551 -0.6740  0.9015  0.7859  0.6387   \n",
       "999  8.3197  1.2937 -1.6657 -3.1165 -1.1774 -1.8673  1.0129 -1.5996  3.5014   \n",
       "\n",
       "         X9  ...    X291    X292    X293    X294    X295    X296    X297  \\\n",
       "0    1.8337  ...  1.4625 -0.5273 -0.7469  0.6998 -0.4887 -0.9524  0.3629   \n",
       "1   -1.2674  ...  3.1674 -3.0499  0.4132  1.6582  1.8960  3.9402  0.2812   \n",
       "2    0.8670  ...  0.3530 -0.8737 -1.1940  1.9316 -6.0107  0.3642 -1.5129   \n",
       "3    2.1793  ...  1.4704  2.4512 -0.8304  0.0790  0.9600  0.5918 -0.9744   \n",
       "4    1.2301  ...  0.5095 -3.4169  0.8112 -2.7943  3.8443  0.1666 -0.5501   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "995 -2.8357  ...  0.6565 -2.2997  1.7963  0.2887 -0.1655 -2.5131  1.1653   \n",
       "996 -1.4289  ... -1.8242  0.7801  0.5300 -0.2795  2.2817  0.3588  2.6076   \n",
       "997 -1.1775  ...  1.0520  1.1615  0.7577 -2.4536 -1.6191 -1.8861 -1.3888   \n",
       "998 -3.1715  ...  1.1602 -4.6499 -0.0869 -1.2168  2.4656 -1.1276 -1.4000   \n",
       "999  1.1492  ... -1.9003 -0.3482  2.0477  0.3222  1.4145 -0.7185  3.7552   \n",
       "\n",
       "       X298    X299    X300  \n",
       "0    0.7396 -0.9543 -1.3848  \n",
       "1    0.2473 -1.6407  0.0612  \n",
       "2   -4.1881  3.0166  1.7286  \n",
       "3   -1.3672 -1.0806 -0.5822  \n",
       "4    1.6320  1.0897 -0.3964  \n",
       "..      ...     ...     ...  \n",
       "995 -0.9547  1.0795  1.0793  \n",
       "996  2.0471  2.4782 -0.9987  \n",
       "997 -1.3644  0.2562 -1.2735  \n",
       "998 -0.8152 -2.3054  1.1570  \n",
       "999 -2.3238 -2.4754 -0.8485  \n",
       "\n",
       "[1000 rows x 301 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_challenge.txt', sep=' ')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X291</th>\n",
       "      <th>X292</th>\n",
       "      <th>X293</th>\n",
       "      <th>X294</th>\n",
       "      <th>X295</th>\n",
       "      <th>X296</th>\n",
       "      <th>X297</th>\n",
       "      <th>X298</th>\n",
       "      <th>X299</th>\n",
       "      <th>X300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.173073</td>\n",
       "      <td>-0.132676</td>\n",
       "      <td>0.044760</td>\n",
       "      <td>0.032654</td>\n",
       "      <td>-0.013809</td>\n",
       "      <td>0.047318</td>\n",
       "      <td>-0.034230</td>\n",
       "      <td>-0.060495</td>\n",
       "      <td>0.059239</td>\n",
       "      <td>-0.072583</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077636</td>\n",
       "      <td>0.106083</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.033208</td>\n",
       "      <td>-0.027669</td>\n",
       "      <td>0.009938</td>\n",
       "      <td>0.101634</td>\n",
       "      <td>-0.013376</td>\n",
       "      <td>-0.075141</td>\n",
       "      <td>-0.056801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.765128</td>\n",
       "      <td>2.604111</td>\n",
       "      <td>1.677774</td>\n",
       "      <td>2.277011</td>\n",
       "      <td>1.737659</td>\n",
       "      <td>1.810612</td>\n",
       "      <td>1.797440</td>\n",
       "      <td>2.229707</td>\n",
       "      <td>2.174966</td>\n",
       "      <td>2.000614</td>\n",
       "      <td>...</td>\n",
       "      <td>1.405488</td>\n",
       "      <td>2.244857</td>\n",
       "      <td>1.645870</td>\n",
       "      <td>1.875765</td>\n",
       "      <td>1.876438</td>\n",
       "      <td>1.694698</td>\n",
       "      <td>2.053888</td>\n",
       "      <td>1.731346</td>\n",
       "      <td>1.748322</td>\n",
       "      <td>1.668313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-14.216900</td>\n",
       "      <td>-7.943100</td>\n",
       "      <td>-5.670100</td>\n",
       "      <td>-7.594400</td>\n",
       "      <td>-6.806800</td>\n",
       "      <td>-6.052300</td>\n",
       "      <td>-5.859000</td>\n",
       "      <td>-6.374000</td>\n",
       "      <td>-7.815700</td>\n",
       "      <td>-6.262400</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.869900</td>\n",
       "      <td>-7.921800</td>\n",
       "      <td>-5.622600</td>\n",
       "      <td>-6.313400</td>\n",
       "      <td>-6.377300</td>\n",
       "      <td>-4.996400</td>\n",
       "      <td>-6.026100</td>\n",
       "      <td>-5.585700</td>\n",
       "      <td>-5.911600</td>\n",
       "      <td>-4.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3.150000</td>\n",
       "      <td>-1.821225</td>\n",
       "      <td>-1.044200</td>\n",
       "      <td>-1.564250</td>\n",
       "      <td>-1.177450</td>\n",
       "      <td>-1.185875</td>\n",
       "      <td>-1.272325</td>\n",
       "      <td>-1.593675</td>\n",
       "      <td>-1.534925</td>\n",
       "      <td>-1.389075</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.039425</td>\n",
       "      <td>-1.445675</td>\n",
       "      <td>-1.055600</td>\n",
       "      <td>-1.258850</td>\n",
       "      <td>-1.170150</td>\n",
       "      <td>-1.094850</td>\n",
       "      <td>-1.301425</td>\n",
       "      <td>-1.233650</td>\n",
       "      <td>-1.257250</td>\n",
       "      <td>-1.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.030150</td>\n",
       "      <td>-0.201750</td>\n",
       "      <td>0.071550</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>-0.014850</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>-0.064100</td>\n",
       "      <td>-0.038950</td>\n",
       "      <td>0.101350</td>\n",
       "      <td>-0.055000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014800</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>-0.037250</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>-0.012400</td>\n",
       "      <td>0.070750</td>\n",
       "      <td>-0.056500</td>\n",
       "      <td>-0.131700</td>\n",
       "      <td>-0.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.376775</td>\n",
       "      <td>1.523550</td>\n",
       "      <td>1.194750</td>\n",
       "      <td>1.591700</td>\n",
       "      <td>1.164500</td>\n",
       "      <td>1.208750</td>\n",
       "      <td>1.201100</td>\n",
       "      <td>1.433375</td>\n",
       "      <td>1.542400</td>\n",
       "      <td>1.255100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938700</td>\n",
       "      <td>1.662050</td>\n",
       "      <td>1.116000</td>\n",
       "      <td>1.277200</td>\n",
       "      <td>1.159675</td>\n",
       "      <td>1.126250</td>\n",
       "      <td>1.566250</td>\n",
       "      <td>1.108475</td>\n",
       "      <td>1.102825</td>\n",
       "      <td>1.069725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16.013400</td>\n",
       "      <td>7.812900</td>\n",
       "      <td>5.488700</td>\n",
       "      <td>6.885300</td>\n",
       "      <td>6.306000</td>\n",
       "      <td>6.120900</td>\n",
       "      <td>4.985200</td>\n",
       "      <td>5.921000</td>\n",
       "      <td>7.406100</td>\n",
       "      <td>6.067000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.682900</td>\n",
       "      <td>6.701100</td>\n",
       "      <td>5.258200</td>\n",
       "      <td>5.778000</td>\n",
       "      <td>5.786300</td>\n",
       "      <td>4.862400</td>\n",
       "      <td>6.730300</td>\n",
       "      <td>5.344500</td>\n",
       "      <td>4.824800</td>\n",
       "      <td>4.994200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 y           X1           X2           X3           X4  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.173073    -0.132676     0.044760     0.032654    -0.013809   \n",
       "std       4.765128     2.604111     1.677774     2.277011     1.737659   \n",
       "min     -14.216900    -7.943100    -5.670100    -7.594400    -6.806800   \n",
       "25%      -3.150000    -1.821225    -1.044200    -1.564250    -1.177450   \n",
       "50%       0.030150    -0.201750     0.071550     0.062700    -0.014850   \n",
       "75%       3.376775     1.523550     1.194750     1.591700     1.164500   \n",
       "max      16.013400     7.812900     5.488700     6.885300     6.306000   \n",
       "\n",
       "                X5           X6           X7           X8           X9  ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
       "mean      0.047318    -0.034230    -0.060495     0.059239    -0.072583  ...   \n",
       "std       1.810612     1.797440     2.229707     2.174966     2.000614  ...   \n",
       "min      -6.052300    -5.859000    -6.374000    -7.815700    -6.262400  ...   \n",
       "25%      -1.185875    -1.272325    -1.593675    -1.534925    -1.389075  ...   \n",
       "50%       0.030000    -0.064100    -0.038950     0.101350    -0.055000  ...   \n",
       "75%       1.208750     1.201100     1.433375     1.542400     1.255100  ...   \n",
       "max       6.120900     4.985200     5.921000     7.406100     6.067000  ...   \n",
       "\n",
       "              X291         X292         X293         X294         X295  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     -0.077636     0.106083     0.000218     0.033208    -0.027669   \n",
       "std       1.405488     2.244857     1.645870     1.875765     1.876438   \n",
       "min      -3.869900    -7.921800    -5.622600    -6.313400    -6.377300   \n",
       "25%      -1.039425    -1.445675    -1.055600    -1.258850    -1.170150   \n",
       "50%      -0.014800     0.109100    -0.037250     0.053300     0.018400   \n",
       "75%       0.938700     1.662050     1.116000     1.277200     1.159675   \n",
       "max       4.682900     6.701100     5.258200     5.778000     5.786300   \n",
       "\n",
       "              X296         X297         X298         X299         X300  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean      0.009938     0.101634    -0.013376    -0.075141    -0.056801  \n",
       "std       1.694698     2.053888     1.731346     1.748322     1.668313  \n",
       "min      -4.996400    -6.026100    -5.585700    -5.911600    -4.977000  \n",
       "25%      -1.094850    -1.301425    -1.233650    -1.257250    -1.172300  \n",
       "50%      -0.012400     0.070750    -0.056500    -0.131700    -0.042700  \n",
       "75%       1.126250     1.566250     1.108475     1.102825     1.069725  \n",
       "max       4.862400     6.730300     5.344500     4.824800     4.994200  \n",
       "\n",
       "[8 rows x 301 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.762745333468939"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean prediction (comme floor de prediction)\n",
    "mean_pred = df.y.mean() * np.ones(shape=df.y.shape)\n",
    "np.sqrt(mean_squared_error(df.y, mean_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_total = df.drop(columns='y')\n",
    "y_total = df.y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X291</th>\n",
       "      <th>X292</th>\n",
       "      <th>X293</th>\n",
       "      <th>X294</th>\n",
       "      <th>X295</th>\n",
       "      <th>X296</th>\n",
       "      <th>X297</th>\n",
       "      <th>X298</th>\n",
       "      <th>X299</th>\n",
       "      <th>X300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-1.7972</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>0.9934</td>\n",
       "      <td>-0.5440</td>\n",
       "      <td>-1.0062</td>\n",
       "      <td>-1.7335</td>\n",
       "      <td>1.5835</td>\n",
       "      <td>2.0648</td>\n",
       "      <td>1.7382</td>\n",
       "      <td>1.0355</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4326</td>\n",
       "      <td>-4.6675</td>\n",
       "      <td>-1.0468</td>\n",
       "      <td>-3.5263</td>\n",
       "      <td>0.4345</td>\n",
       "      <td>-1.3600</td>\n",
       "      <td>0.7413</td>\n",
       "      <td>2.1686</td>\n",
       "      <td>-0.2505</td>\n",
       "      <td>2.1406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>2.0045</td>\n",
       "      <td>-4.4152</td>\n",
       "      <td>3.4917</td>\n",
       "      <td>-1.0867</td>\n",
       "      <td>-0.7944</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>-4.3240</td>\n",
       "      <td>-0.9936</td>\n",
       "      <td>-1.3720</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.8667</td>\n",
       "      <td>-1.1814</td>\n",
       "      <td>1.4515</td>\n",
       "      <td>0.3956</td>\n",
       "      <td>5.5099</td>\n",
       "      <td>0.1971</td>\n",
       "      <td>2.1381</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>-2.7908</td>\n",
       "      <td>1.8002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>-1.4394</td>\n",
       "      <td>1.7452</td>\n",
       "      <td>-0.2411</td>\n",
       "      <td>-2.4647</td>\n",
       "      <td>-0.9146</td>\n",
       "      <td>2.6722</td>\n",
       "      <td>-1.2209</td>\n",
       "      <td>1.2478</td>\n",
       "      <td>-3.6149</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>-0.3301</td>\n",
       "      <td>1.8298</td>\n",
       "      <td>-1.1579</td>\n",
       "      <td>-0.3797</td>\n",
       "      <td>0.5904</td>\n",
       "      <td>1.9959</td>\n",
       "      <td>0.0504</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>-0.2904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>-3.3637</td>\n",
       "      <td>2.5289</td>\n",
       "      <td>-1.2437</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>1.4713</td>\n",
       "      <td>-1.4912</td>\n",
       "      <td>3.1778</td>\n",
       "      <td>-0.3050</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2468</td>\n",
       "      <td>0.4632</td>\n",
       "      <td>0.8648</td>\n",
       "      <td>1.5815</td>\n",
       "      <td>-0.8711</td>\n",
       "      <td>-0.1695</td>\n",
       "      <td>0.4415</td>\n",
       "      <td>3.5580</td>\n",
       "      <td>-2.5576</td>\n",
       "      <td>-0.6236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>4.6066</td>\n",
       "      <td>-0.0678</td>\n",
       "      <td>0.9489</td>\n",
       "      <td>0.6852</td>\n",
       "      <td>-1.4747</td>\n",
       "      <td>0.9723</td>\n",
       "      <td>-0.8049</td>\n",
       "      <td>-2.2480</td>\n",
       "      <td>0.9982</td>\n",
       "      <td>-2.6351</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3356</td>\n",
       "      <td>2.4517</td>\n",
       "      <td>-2.4740</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-0.3025</td>\n",
       "      <td>-0.2332</td>\n",
       "      <td>-4.9751</td>\n",
       "      <td>1.6519</td>\n",
       "      <td>4.6908</td>\n",
       "      <td>-4.7017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>-1.4215</td>\n",
       "      <td>1.2035</td>\n",
       "      <td>1.2568</td>\n",
       "      <td>-2.4495</td>\n",
       "      <td>-2.8401</td>\n",
       "      <td>-1.1208</td>\n",
       "      <td>-0.7124</td>\n",
       "      <td>0.7779</td>\n",
       "      <td>1.1640</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7492</td>\n",
       "      <td>1.4950</td>\n",
       "      <td>-0.9452</td>\n",
       "      <td>1.1867</td>\n",
       "      <td>-0.6317</td>\n",
       "      <td>-1.0455</td>\n",
       "      <td>-0.1326</td>\n",
       "      <td>0.7198</td>\n",
       "      <td>-0.0225</td>\n",
       "      <td>-1.8262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>-1.0438</td>\n",
       "      <td>-0.2305</td>\n",
       "      <td>3.5178</td>\n",
       "      <td>2.0623</td>\n",
       "      <td>0.2037</td>\n",
       "      <td>-3.2167</td>\n",
       "      <td>2.8555</td>\n",
       "      <td>-0.5448</td>\n",
       "      <td>-1.7315</td>\n",
       "      <td>-3.0366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>-0.3745</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>1.1129</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>1.0800</td>\n",
       "      <td>-1.6079</td>\n",
       "      <td>1.0062</td>\n",
       "      <td>3.8849</td>\n",
       "      <td>0.7253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>-0.1442</td>\n",
       "      <td>-0.8724</td>\n",
       "      <td>2.6570</td>\n",
       "      <td>3.7338</td>\n",
       "      <td>3.1332</td>\n",
       "      <td>1.3293</td>\n",
       "      <td>-0.6909</td>\n",
       "      <td>-0.5979</td>\n",
       "      <td>-3.2334</td>\n",
       "      <td>-4.0216</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4960</td>\n",
       "      <td>0.4626</td>\n",
       "      <td>3.3065</td>\n",
       "      <td>-0.1816</td>\n",
       "      <td>3.4538</td>\n",
       "      <td>2.8346</td>\n",
       "      <td>0.3760</td>\n",
       "      <td>-0.2995</td>\n",
       "      <td>-1.1550</td>\n",
       "      <td>-0.4645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>-0.3173</td>\n",
       "      <td>1.7513</td>\n",
       "      <td>0.8805</td>\n",
       "      <td>-0.8513</td>\n",
       "      <td>1.3458</td>\n",
       "      <td>1.7845</td>\n",
       "      <td>-3.7659</td>\n",
       "      <td>-2.6441</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.1014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4122</td>\n",
       "      <td>4.1780</td>\n",
       "      <td>-0.0704</td>\n",
       "      <td>4.3997</td>\n",
       "      <td>-1.1846</td>\n",
       "      <td>-1.0083</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>1.4541</td>\n",
       "      <td>-0.2211</td>\n",
       "      <td>2.0144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>-0.5181</td>\n",
       "      <td>1.8747</td>\n",
       "      <td>1.5320</td>\n",
       "      <td>-0.2039</td>\n",
       "      <td>-3.0017</td>\n",
       "      <td>-2.0958</td>\n",
       "      <td>-1.7080</td>\n",
       "      <td>1.0649</td>\n",
       "      <td>2.6926</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9964</td>\n",
       "      <td>1.3498</td>\n",
       "      <td>-3.5224</td>\n",
       "      <td>2.0811</td>\n",
       "      <td>1.9952</td>\n",
       "      <td>-1.5158</td>\n",
       "      <td>-0.8341</td>\n",
       "      <td>3.1930</td>\n",
       "      <td>0.9357</td>\n",
       "      <td>-2.4483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1      X2      X3      X4      X5      X6      X7      X8      X9  \\\n",
       "68  -1.7972  0.2872  0.9934 -0.5440 -1.0062 -1.7335  1.5835  2.0648  1.7382   \n",
       "895  2.0045 -4.4152  3.4917 -1.0867 -0.7944  0.9782 -4.3240 -0.9936 -1.3720   \n",
       "657 -1.4394  1.7452 -0.2411 -2.4647 -0.9146  2.6722 -1.2209  1.2478 -3.6149   \n",
       "760 -3.3637  2.5289 -1.2437  0.1755  0.0418  0.9973  1.4713 -1.4912  3.1778   \n",
       "994  4.6066 -0.0678  0.9489  0.6852 -1.4747  0.9723 -0.8049 -2.2480  0.9982   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "469 -1.4215  1.2035  1.2568 -2.4495 -2.8401 -1.1208 -0.7124  0.7779  1.1640   \n",
       "432 -1.0438 -0.2305  3.5178  2.0623  0.2037 -3.2167  2.8555 -0.5448 -1.7315   \n",
       "743 -0.1442 -0.8724  2.6570  3.7338  3.1332  1.3293 -0.6909 -0.5979 -3.2334   \n",
       "439 -0.3173  1.7513  0.8805 -0.8513  1.3458  1.7845 -3.7659 -2.6441  0.4083   \n",
       "781  1.5603 -0.5181  1.8747  1.5320 -0.2039 -3.0017 -2.0958 -1.7080  1.0649   \n",
       "\n",
       "        X10  ...    X291    X292    X293    X294    X295    X296    X297  \\\n",
       "68   1.0355  ...  2.4326 -4.6675 -1.0468 -3.5263  0.4345 -1.3600  0.7413   \n",
       "895 -0.4779  ... -0.8667 -1.1814  1.4515  0.3956  5.5099  0.1971  2.1381   \n",
       "657  0.0645  ...  0.1704 -0.3301  1.8298 -1.1579 -0.3797  0.5904  1.9959   \n",
       "760 -0.3050  ...  1.2468  0.4632  0.8648  1.5815 -0.8711 -0.1695  0.4415   \n",
       "994 -2.6351  ...  1.3356  2.4517 -2.4740 -0.2187 -0.3025 -0.2332 -4.9751   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "469  0.0045  ... -0.7492  1.4950 -0.9452  1.1867 -0.6317 -1.0455 -0.1326   \n",
       "432 -3.0366  ...  0.7845 -0.3745  0.0036  1.1129 -0.0093  1.0800 -1.6079   \n",
       "743 -4.0216  ...  2.4960  0.4626  3.3065 -0.1816  3.4538  2.8346  0.3760   \n",
       "439  0.1014  ...  0.4122  4.1780 -0.0704  4.3997 -1.1846 -1.0083  0.1111   \n",
       "781  2.6926  ...  1.9964  1.3498 -3.5224  2.0811  1.9952 -1.5158 -0.8341   \n",
       "\n",
       "       X298    X299    X300  \n",
       "68   2.1686 -0.2505  2.1406  \n",
       "895  0.0363 -2.7908  1.8002  \n",
       "657  0.0504 -0.3743 -0.2904  \n",
       "760  3.5580 -2.5576 -0.6236  \n",
       "994  1.6519  4.6908 -4.7017  \n",
       "..      ...     ...     ...  \n",
       "469  0.7198 -0.0225 -1.8262  \n",
       "432  1.0062  3.8849  0.7253  \n",
       "743 -0.2995 -1.1550 -0.4645  \n",
       "439  1.4541 -0.2211  2.0144  \n",
       "781  3.1930  0.9357 -2.4483  \n",
       "\n",
       "[750 rows x 300 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y       1.000000\n",
       "X278    0.544866\n",
       "X90     0.484006\n",
       "X259    0.425620\n",
       "X241    0.404377\n",
       "          ...   \n",
       "X37     0.002024\n",
       "X211    0.002006\n",
       "X178    0.000538\n",
       "X150    0.000371\n",
       "X223    0.000160\n",
       "Name: y, Length: 301, dtype: float64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.corr().y).abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.30256758101439407\n",
      "coefs: [-1.33963955]\n",
      "intercept: 0.20620456239669188\n",
      "rmse: 4.0302908273528715\n"
     ]
    }
   ],
   "source": [
    "# regression lineaire simple\n",
    "\n",
    "rls = LinearRegression().fit(X_train.X278.to_numpy().reshape(-1,1), y_train)\n",
    "\n",
    "print('score:', rls.score(X_train.X278.to_numpy().reshape(-1,1), y_train))\n",
    "print('coefs:', rls.coef_)\n",
    "print('intercept:', rls.intercept_)\n",
    "\n",
    "preds = rls.predict(X_test.X278.to_numpy().reshape(-1,1))\n",
    "print('rmse:', np.sqrt(mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.971\n",
      "Model:                            OLS   Adj. R-squared:                  0.951\n",
      "Method:                 Least Squares   F-statistic:                     49.72\n",
      "Date:                Sun, 06 Nov 2022   Prob (F-statistic):          2.30e-239\n",
      "Time:                        22:06:46   Log-Likelihood:                -911.20\n",
      "No. Observations:                 750   AIC:                             2424.\n",
      "Df Residuals:                     449   BIC:                             3815.\n",
      "Df Model:                         300                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0607      0.049     -1.250      0.212      -0.156       0.035\n",
      "X1            -0.3424      4.451     -0.077      0.939      -9.090       8.405\n",
      "X2             2.5723      4.898      0.525      0.600      -7.053      12.197\n",
      "X3            -7.5018      4.421     -1.697      0.090     -16.191       1.187\n",
      "X4            -5.4664      5.322     -1.027      0.305     -15.925       4.992\n",
      "X5             1.4907      4.932      0.302      0.763      -8.202      11.183\n",
      "X6             2.4254      4.694      0.517      0.606      -6.799      11.650\n",
      "X7             7.0256      4.789      1.467      0.143      -2.385      16.436\n",
      "X8            -5.1594      4.548     -1.134      0.257     -14.098       3.779\n",
      "X9            -0.4595      4.793     -0.096      0.924      -9.879       8.960\n",
      "X10           -0.7286      4.565     -0.160      0.873      -9.700       8.243\n",
      "X11           -4.4671      4.885     -0.914      0.361     -14.068       5.134\n",
      "X12            2.4730      5.068      0.488      0.626      -7.486      12.432\n",
      "X13           -4.5015      4.808     -0.936      0.350     -13.951       4.948\n",
      "X14            4.3226      4.687      0.922      0.357      -4.889      13.534\n",
      "X15            4.2621      4.816      0.885      0.377      -5.203      13.727\n",
      "X16           -2.3930      4.667     -0.513      0.608     -11.565       6.779\n",
      "X17           -3.8630      4.539     -0.851      0.395     -12.784       5.058\n",
      "X18           -1.2138      4.712     -0.258      0.797     -10.474       8.046\n",
      "X19           -0.6037      4.767     -0.127      0.899      -9.972       8.765\n",
      "X20           -4.6118      4.595     -1.004      0.316     -13.642       4.419\n",
      "X21           -3.0092      4.671     -0.644      0.520     -12.188       6.170\n",
      "X22           10.6046      4.861      2.182      0.030       1.052      20.157\n",
      "X23           -0.9955      4.557     -0.218      0.827      -9.952       7.961\n",
      "X24            2.2011      4.422      0.498      0.619      -6.489      10.891\n",
      "X25            6.4558      4.612      1.400      0.162      -2.609      15.520\n",
      "X26            5.2593      4.835      1.088      0.277      -4.243      14.762\n",
      "X27           -6.6575      5.065     -1.315      0.189     -16.611       3.296\n",
      "X28           -0.6373      4.859     -0.131      0.896     -10.187       8.913\n",
      "X29           -2.0986      4.211     -0.498      0.619     -10.375       6.178\n",
      "X30            0.8454      4.678      0.181      0.857      -8.349      10.040\n",
      "X31            4.9707      4.360      1.140      0.255      -3.598      13.539\n",
      "X32           -4.7076      4.734     -0.994      0.321     -14.011       4.595\n",
      "X33           -1.3770      4.578     -0.301      0.764     -10.374       7.620\n",
      "X34           -0.8164      4.359     -0.187      0.852      -9.384       7.751\n",
      "X35            0.4404      4.505      0.098      0.922      -8.413       9.293\n",
      "X36           -9.6046      4.547     -2.112      0.035     -18.541      -0.668\n",
      "X37            1.6119      5.136      0.314      0.754      -8.482      11.706\n",
      "X38           -0.0579      4.887     -0.012      0.991      -9.662       9.546\n",
      "X39            3.8093      4.726      0.806      0.421      -5.479      13.098\n",
      "X40           -0.7392      4.699     -0.157      0.875      -9.974       8.496\n",
      "X41            5.7025      4.658      1.224      0.221      -3.451      14.856\n",
      "X42           -5.4565      4.608     -1.184      0.237     -14.513       3.600\n",
      "X43           -3.9643      4.654     -0.852      0.395     -13.111       5.182\n",
      "X44            8.9421      4.522      1.977      0.049       0.055      17.829\n",
      "X45            3.8318      4.843      0.791      0.429      -5.685      13.349\n",
      "X46            3.6794      4.987      0.738      0.461      -6.122      13.481\n",
      "X47            5.7933      4.591      1.262      0.208      -3.228      14.815\n",
      "X48           -3.9966      4.890     -0.817      0.414     -13.607       5.613\n",
      "X49           -6.1377      5.032     -1.220      0.223     -16.026       3.751\n",
      "X50           -3.1612      4.814     -0.657      0.512     -12.622       6.300\n",
      "X51           -1.6350      4.800     -0.341      0.734     -11.067       7.797\n",
      "X52            2.3118      4.596      0.503      0.615      -6.721      11.344\n",
      "X53            4.9217      4.693      1.049      0.295      -4.302      14.145\n",
      "X54            1.4751      4.783      0.308      0.758      -7.925      10.875\n",
      "X55           -6.6057      4.537     -1.456      0.146     -15.523       2.311\n",
      "X56           -5.9096      4.652     -1.270      0.205     -15.051       3.232\n",
      "X57           -3.2772      5.156     -0.636      0.525     -13.411       6.857\n",
      "X58           -0.7327      4.471     -0.164      0.870      -9.519       8.054\n",
      "X59           -6.2682      4.713     -1.330      0.184     -15.530       2.994\n",
      "X60           10.5568      4.545      2.323      0.021       1.625      19.489\n",
      "X61           -1.6875      4.674     -0.361      0.718     -10.873       7.498\n",
      "X62           -2.8568      4.577     -0.624      0.533     -11.851       6.138\n",
      "X63           14.8947      4.570      3.259      0.001       5.913      23.877\n",
      "X64           -7.3209      4.626     -1.582      0.114     -16.413       1.771\n",
      "X65            3.4380      4.614      0.745      0.457      -5.630      12.506\n",
      "X66           -7.3277      4.613     -1.588      0.113     -16.394       1.738\n",
      "X67            8.5210      4.692      1.816      0.070      -0.699      17.741\n",
      "X68            4.3461      4.748      0.915      0.360      -4.985      13.677\n",
      "X69           -2.2785      4.676     -0.487      0.626     -11.469       6.912\n",
      "X70            0.3287      4.779      0.069      0.945      -9.063       9.721\n",
      "X71           -1.0374      4.841     -0.214      0.830     -10.552       8.477\n",
      "X72            0.2842      4.799      0.059      0.953      -9.147       9.716\n",
      "X73            0.4124      4.578      0.090      0.928      -8.585       9.410\n",
      "X74           -2.6904      4.804     -0.560      0.576     -12.132       6.751\n",
      "X75            0.3063      4.969      0.062      0.951      -9.459      10.072\n",
      "X76            4.1213      4.831      0.853      0.394      -5.373      13.615\n",
      "X77            3.9399      4.711      0.836      0.403      -5.318      13.197\n",
      "X78           -3.5376      4.771     -0.741      0.459     -12.914       5.839\n",
      "X79           -0.0527      4.593     -0.011      0.991      -9.079       8.974\n",
      "X80            3.9002      4.528      0.861      0.389      -4.998      12.798\n",
      "X81            3.7017      4.915      0.753      0.452      -5.957      13.360\n",
      "X82           -3.9406      4.648     -0.848      0.397     -13.076       5.194\n",
      "X83           -0.5827      4.606     -0.127      0.899      -9.635       8.469\n",
      "X84            5.4771      4.491      1.219      0.223      -3.350      14.304\n",
      "X85           -0.0884      4.978     -0.018      0.986      -9.871       9.694\n",
      "X86            0.6256      4.726      0.132      0.895      -8.662       9.913\n",
      "X87            0.4865      4.508      0.108      0.914      -8.373       9.346\n",
      "X88           -1.2255      4.420     -0.277      0.782      -9.912       7.461\n",
      "X89           -0.7301      4.835     -0.151      0.880     -10.232       8.772\n",
      "X90           -3.1890      4.886     -0.653      0.514     -12.792       6.414\n",
      "X91           -7.2087      4.815     -1.497      0.135     -16.671       2.254\n",
      "X92            4.7574      4.786      0.994      0.321      -4.649      14.164\n",
      "X93           -3.0470      4.603     -0.662      0.508     -12.093       5.999\n",
      "X94            3.1201      4.681      0.667      0.505      -6.079      12.319\n",
      "X95            1.3087      4.542      0.288      0.773      -7.619      10.236\n",
      "X96           -0.6110      4.596     -0.133      0.894      -9.644       8.422\n",
      "X97           -4.7526      4.537     -1.048      0.295     -13.668       4.163\n",
      "X98           -4.6525      4.755     -0.978      0.328     -13.997       4.692\n",
      "X99            0.3724      4.630      0.080      0.936      -8.726       9.471\n",
      "X100          -5.4350      4.835     -1.124      0.262     -14.938       4.068\n",
      "X101          -1.5762      4.843     -0.325      0.745     -11.094       7.941\n",
      "X102          -0.7846      4.851     -0.162      0.872     -10.319       8.749\n",
      "X103           1.8800      4.812      0.391      0.696      -7.577      11.337\n",
      "X104          -1.5010      4.800     -0.313      0.755     -10.935       7.933\n",
      "X105           1.8652      4.686      0.398      0.691      -7.344      11.075\n",
      "X106           2.5414      4.797      0.530      0.596      -6.885      11.968\n",
      "X107          -7.2396      4.825     -1.500      0.134     -16.722       2.243\n",
      "X108           0.1631      4.973      0.033      0.974      -9.610       9.936\n",
      "X109          -3.9033      4.519     -0.864      0.388     -12.785       4.979\n",
      "X110          -8.9524      4.888     -1.831      0.068     -18.559       0.654\n",
      "X111          -5.3650      4.792     -1.120      0.263     -14.782       4.052\n",
      "X112          -0.5044      4.745     -0.106      0.915      -9.830       8.821\n",
      "X113          -1.7541      5.035     -0.348      0.728     -11.650       8.142\n",
      "X114         -10.5022      4.540     -2.313      0.021     -19.424      -1.581\n",
      "X115          -8.3992      4.674     -1.797      0.073     -17.584       0.786\n",
      "X116           3.8764      4.754      0.815      0.415      -5.466      13.219\n",
      "X117          -7.2951      4.737     -1.540      0.124     -16.605       2.015\n",
      "X118          -3.2348      4.667     -0.693      0.489     -12.406       5.937\n",
      "X119           1.9306      4.518      0.427      0.669      -6.948      10.809\n",
      "X120           2.2344      4.686      0.477      0.634      -6.975      11.444\n",
      "X121          -7.2765      4.842     -1.503      0.134     -16.792       2.239\n",
      "X122          -2.4817      4.693     -0.529      0.597     -11.705       6.741\n",
      "X123           2.7932      4.841      0.577      0.564      -6.721      12.307\n",
      "X124          -1.0776      4.316     -0.250      0.803      -9.560       7.405\n",
      "X125           5.7324      4.686      1.223      0.222      -3.477      14.942\n",
      "X126          -3.2271      5.265     -0.613      0.540     -13.574       7.120\n",
      "X127           5.7234      4.458      1.284      0.200      -3.037      14.484\n",
      "X128          -7.6871      4.986     -1.542      0.124     -17.486       2.112\n",
      "X129           0.8742      4.696      0.186      0.852      -8.354      10.102\n",
      "X130           8.1502      4.715      1.728      0.085      -1.116      17.417\n",
      "X131          -2.4840      4.435     -0.560      0.576     -11.201       6.232\n",
      "X132          -0.1674      4.832     -0.035      0.972      -9.664       9.329\n",
      "X133           5.3893      4.396      1.226      0.221      -3.250      14.028\n",
      "X134           2.6369      5.012      0.526      0.599      -7.212      12.486\n",
      "X135           2.2258      4.852      0.459      0.647      -7.309      11.761\n",
      "X136           8.4119      4.635      1.815      0.070      -0.697      17.521\n",
      "X137          -6.5434      4.800     -1.363      0.174     -15.977       2.890\n",
      "X138           5.6609      4.547      1.245      0.214      -3.275      14.596\n",
      "X139           0.7109      4.590      0.155      0.877      -8.309       9.731\n",
      "X140          -0.6343      4.667     -0.136      0.892      -9.807       8.538\n",
      "X141          -0.8966      4.929     -0.182      0.856     -10.583       8.790\n",
      "X142          12.1970      4.865      2.507      0.013       2.636      21.758\n",
      "X143          -0.4169      4.742     -0.088      0.930      -9.736       8.902\n",
      "X144          -2.0699      4.254     -0.487      0.627     -10.430       6.290\n",
      "X145          -7.8647      4.805     -1.637      0.102     -17.308       1.578\n",
      "X146           7.1827      4.883      1.471      0.142      -2.413      16.779\n",
      "X147           2.9358      4.810      0.610      0.542      -6.516      12.388\n",
      "X148          -2.3979      4.720     -0.508      0.612     -11.674       6.878\n",
      "X149           1.5420      4.757      0.324      0.746      -7.806      10.890\n",
      "X150          -0.7563      4.583     -0.165      0.869      -9.764       8.251\n",
      "X151           3.7088      4.566      0.812      0.417      -5.265      12.683\n",
      "X152           1.3845      4.613      0.300      0.764      -7.682      10.451\n",
      "X153          -2.6752      4.585     -0.583      0.560     -11.686       6.336\n",
      "X154          -3.3772      4.735     -0.713      0.476     -12.683       5.929\n",
      "X155           2.8905      4.673      0.619      0.537      -6.294      12.075\n",
      "X156           7.8302      4.585      1.708      0.088      -1.182      16.842\n",
      "X157          -5.0738      4.819     -1.053      0.293     -14.544       4.397\n",
      "X158           3.5026      4.414      0.793      0.428      -5.173      12.178\n",
      "X159         -13.5716      4.854     -2.796      0.005     -23.111      -4.032\n",
      "X160          -2.5640      5.076     -0.505      0.614     -12.540       7.412\n",
      "X161           1.3565      4.557      0.298      0.766      -7.600      10.313\n",
      "X162          -5.9487      4.869     -1.222      0.222     -15.517       3.620\n",
      "X163          -2.4913      4.967     -0.502      0.616     -12.252       7.269\n",
      "X164           5.0961      4.657      1.094      0.274      -4.056      14.248\n",
      "X165          -2.8634      4.613     -0.621      0.535     -11.929       6.202\n",
      "X166          -5.1520      4.522     -1.139      0.255     -14.039       3.735\n",
      "X167           3.2247      4.637      0.695      0.487      -5.888      12.338\n",
      "X168           5.8032      4.408      1.316      0.189      -2.860      14.467\n",
      "X169          -0.1729      4.867     -0.036      0.972      -9.738       9.393\n",
      "X170           0.9476      4.785      0.198      0.843      -8.455      10.350\n",
      "X171           4.4470      4.719      0.942      0.347      -4.828      13.722\n",
      "X172           4.0213      5.254      0.765      0.444      -6.303      14.346\n",
      "X173          -3.1977      4.783     -0.668      0.504     -12.598       6.203\n",
      "X174           1.5563      4.642      0.335      0.738      -7.567      10.679\n",
      "X175           6.7278      4.536      1.483      0.139      -2.186      15.641\n",
      "X176           0.6292      4.688      0.134      0.893      -8.583       9.842\n",
      "X177          -3.6689      4.646     -0.790      0.430     -12.800       5.462\n",
      "X178           0.2368      4.434      0.053      0.957      -8.476       8.950\n",
      "X179          -0.2917      4.537     -0.064      0.949      -9.208       8.625\n",
      "X180          -0.0755      4.842     -0.016      0.988      -9.592       9.441\n",
      "X181          -4.2009      5.031     -0.835      0.404     -14.087       5.686\n",
      "X182          -2.2184      4.867     -0.456      0.649     -11.783       7.346\n",
      "X183          -1.7577      4.253     -0.413      0.680     -10.116       6.600\n",
      "X184           3.8458      4.720      0.815      0.416      -5.431      13.122\n",
      "X185          -2.0095      4.441     -0.452      0.651     -10.738       6.719\n",
      "X186           2.2331      4.699      0.475      0.635      -7.002      11.468\n",
      "X187          -2.6785      4.662     -0.575      0.566     -11.841       6.484\n",
      "X188          -1.1376      4.725     -0.241      0.810     -10.423       8.148\n",
      "X189           0.7774      4.726      0.164      0.869      -8.510      10.065\n",
      "X190           1.5054      4.524      0.333      0.739      -7.385      10.396\n",
      "X191           0.1184      4.736      0.025      0.980      -9.190       9.426\n",
      "X192          -1.4770      4.749     -0.311      0.756     -10.810       7.856\n",
      "X193          -5.9907      4.654     -1.287      0.199     -15.137       3.156\n",
      "X194          10.9333      4.698      2.327      0.020       1.701      20.166\n",
      "X195         -10.6057      4.641     -2.285      0.023     -19.726      -1.485\n",
      "X196           2.4153      4.854      0.498      0.619      -7.125      11.956\n",
      "X197           1.1527      4.649      0.248      0.804      -7.984      10.289\n",
      "X198          -4.4254      4.634     -0.955      0.340     -13.532       4.681\n",
      "X199           0.5191      4.634      0.112      0.911      -8.587       9.626\n",
      "X200           4.4825      4.842      0.926      0.355      -5.034      13.999\n",
      "X201           7.8278      4.524      1.730      0.084      -1.063      16.719\n",
      "X202          -3.1438      5.047     -0.623      0.534     -13.062       6.775\n",
      "X203           3.0617      4.677      0.655      0.513      -6.129      12.253\n",
      "X204          -0.4518      4.649     -0.097      0.923      -9.587       8.684\n",
      "X205           2.1467      4.591      0.468      0.640      -6.875      11.168\n",
      "X206           5.0152      4.688      1.070      0.285      -4.197      14.228\n",
      "X207          -6.8851      4.449     -1.548      0.122     -15.628       1.857\n",
      "X208          -2.1624      4.757     -0.455      0.650     -11.512       7.187\n",
      "X209          -7.0364      4.705     -1.496      0.135     -16.282       2.209\n",
      "X210           4.4930      4.490      1.001      0.318      -4.332      13.318\n",
      "X211          -0.1078      4.527     -0.024      0.981      -9.005       8.790\n",
      "X212          -3.4408      4.783     -0.719      0.472     -12.840       5.959\n",
      "X213           3.1525      4.924      0.640      0.522      -6.524      12.829\n",
      "X214           1.7094      4.681      0.365      0.715      -7.490      10.909\n",
      "X215          -3.5077      4.975     -0.705      0.481     -13.284       6.269\n",
      "X216           1.3319      4.838      0.275      0.783      -8.177      10.841\n",
      "X217          -6.7124      4.810     -1.395      0.164     -16.166       2.741\n",
      "X218           3.7218      4.697      0.792      0.429      -5.509      12.953\n",
      "X219           2.1476      4.344      0.494      0.621      -6.390      10.685\n",
      "X220          -4.3878      4.608     -0.952      0.342     -13.444       4.669\n",
      "X221           6.0299      4.712      1.280      0.201      -3.230      15.290\n",
      "X222           0.0196      5.237      0.004      0.997     -10.272      10.311\n",
      "X223           2.8680      4.646      0.617      0.537      -6.262      11.998\n",
      "X224           1.7542      4.804      0.365      0.715      -7.687      11.196\n",
      "X225          -1.6914      4.494     -0.376      0.707     -10.523       7.140\n",
      "X226           0.0476      4.688      0.010      0.992      -9.166       9.261\n",
      "X227           2.3660      4.737      0.499      0.618      -6.944      11.676\n",
      "X228           2.4795      4.686      0.529      0.597      -6.731      11.690\n",
      "X229          -3.5271      4.732     -0.745      0.456     -12.826       5.772\n",
      "X230          -4.8477      4.674     -1.037      0.300     -14.033       4.338\n",
      "X231          -4.2521      4.625     -0.919      0.358     -13.341       4.837\n",
      "X232          -1.2729      4.565     -0.279      0.780     -10.244       7.698\n",
      "X233           2.3291      4.724      0.493      0.622      -6.955      11.613\n",
      "X234           3.6314      4.967      0.731      0.465      -6.129      13.392\n",
      "X235           1.7750      4.252      0.417      0.677      -6.581      10.131\n",
      "X236          -1.3944      4.658     -0.299      0.765     -10.549       7.761\n",
      "X237           4.9317      4.598      1.073      0.284      -4.105      13.968\n",
      "X238           0.5641      4.821      0.117      0.907      -8.910      10.039\n",
      "X239           4.0541      4.756      0.852      0.394      -5.293      13.401\n",
      "X240          -7.2304      4.673     -1.547      0.122     -16.413       1.952\n",
      "X241          -3.7490      4.844     -0.774      0.439     -13.268       5.770\n",
      "X242           3.2841      4.684      0.701      0.484      -5.921      12.490\n",
      "X243           4.0817      4.415      0.925      0.356      -4.595      12.758\n",
      "X244           0.5389      4.863      0.111      0.912      -9.018      10.096\n",
      "X245           2.7350      4.513      0.606      0.545      -6.134      11.604\n",
      "X246           0.1230      4.672      0.026      0.979      -9.058       9.304\n",
      "X247          -4.7099      4.697     -1.003      0.317     -13.941       4.521\n",
      "X248          -3.7598      4.683     -0.803      0.423     -12.964       5.444\n",
      "X249           3.8447      5.115      0.752      0.453      -6.208      13.897\n",
      "X250          -6.5337      4.825     -1.354      0.176     -16.016       2.949\n",
      "X251           0.9829      4.859      0.202      0.840      -8.566      10.531\n",
      "X252          -4.8731      4.764     -1.023      0.307     -14.236       4.490\n",
      "X253          -4.4483      4.653     -0.956      0.340     -13.592       4.695\n",
      "X254           6.3643      4.846      1.313      0.190      -3.160      15.888\n",
      "X255         -11.2356      4.917     -2.285      0.023     -20.899      -1.572\n",
      "X256          -0.0788      4.607     -0.017      0.986      -9.134       8.976\n",
      "X257          -0.0897      4.670     -0.019      0.985      -9.267       9.087\n",
      "X258          -9.7728      4.699     -2.080      0.038     -19.007      -0.538\n",
      "X259           0.0822      4.986      0.016      0.987      -9.716       9.880\n",
      "X260          -1.6125      4.580     -0.352      0.725     -10.613       7.388\n",
      "X261          -0.1721      4.971     -0.035      0.972      -9.942       9.598\n",
      "X262          -0.5886      4.673     -0.126      0.900      -9.772       8.595\n",
      "X263           1.5929      4.520      0.352      0.725      -7.290      10.476\n",
      "X264          -0.0386      4.632     -0.008      0.993      -9.141       9.064\n",
      "X265          -3.7217      4.847     -0.768      0.443     -13.248       5.804\n",
      "X266          -1.0798      4.856     -0.222      0.824     -10.623       8.464\n",
      "X267          -1.1997      4.713     -0.255      0.799     -10.463       8.063\n",
      "X268           0.0988      4.690      0.021      0.983      -9.118       9.316\n",
      "X269          -0.8591      4.740     -0.181      0.856     -10.175       8.456\n",
      "X270          -0.7193      4.675     -0.154      0.878      -9.906       8.468\n",
      "X271          -8.1722      4.697     -1.740      0.083     -17.402       1.058\n",
      "X272          -7.4910      4.642     -1.614      0.107     -16.615       1.632\n",
      "X273           2.3305      4.845      0.481      0.631      -7.192      11.853\n",
      "X274           1.3647      4.736      0.288      0.773      -7.942      10.671\n",
      "X275           1.3235      4.755      0.278      0.781      -8.021      10.668\n",
      "X276           1.1400      4.667      0.244      0.807      -8.032      10.312\n",
      "X277           3.3076      5.042      0.656      0.512      -6.601      13.216\n",
      "X278          -0.0939      4.358     -0.022      0.983      -8.659       8.471\n",
      "X279           4.0182      4.514      0.890      0.374      -4.854      12.890\n",
      "X280           0.6878      4.787      0.144      0.886      -8.720      10.096\n",
      "X281          -6.6609      4.741     -1.405      0.161     -15.979       2.657\n",
      "X282           2.4295      4.769      0.509      0.611      -6.943      11.802\n",
      "X283          -3.7813      4.850     -0.780      0.436     -13.314       5.751\n",
      "X284          -6.9161      4.662     -1.484      0.139     -16.078       2.246\n",
      "X285           3.4221      4.578      0.747      0.455      -5.576      12.420\n",
      "X286          -7.0279      4.599     -1.528      0.127     -16.067       2.011\n",
      "X287          -0.1519      4.776     -0.032      0.975      -9.539       9.235\n",
      "X288          11.2819      4.792      2.355      0.019       1.865      20.699\n",
      "X289           1.8583      4.524      0.411      0.681      -7.032      10.749\n",
      "X290          -5.6639      4.939     -1.147      0.252     -15.370       4.043\n",
      "X291          -2.8151      4.782     -0.589      0.556     -12.213       6.583\n",
      "X292           6.3407      4.572      1.387      0.166      -2.644      15.325\n",
      "X293          -3.6581      4.594     -0.796      0.426     -12.686       5.370\n",
      "X294           3.2412      4.861      0.667      0.505      -6.313      12.795\n",
      "X295          -0.0480      4.653     -0.010      0.992      -9.193       9.097\n",
      "X296          -4.8164      4.775     -1.009      0.314     -14.201       4.568\n",
      "X297          -1.6279      4.763     -0.342      0.733     -10.989       7.733\n",
      "X298           1.0714      4.735      0.226      0.821      -8.234      10.377\n",
      "X299          -1.1164      4.725     -0.236      0.813     -10.402       8.169\n",
      "X300          -4.1476      4.718     -0.879      0.380     -13.420       5.125\n",
      "==============================================================================\n",
      "Omnibus:                        0.565   Durbin-Watson:                   2.099\n",
      "Prob(Omnibus):                  0.754   Jarque-Bera (JB):                0.659\n",
      "Skew:                           0.049   Prob(JB):                        0.719\n",
      "Kurtosis:                       2.893   Cond. No.                     1.95e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.95e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X2 = sm.add_constant(X_train)\n",
    "est = sm.OLS(y_train, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9707788524973446\n",
      "rmse: 1.217785498287447\n"
     ]
    }
   ],
   "source": [
    "# regression lineaire multiple\n",
    "\n",
    "rlm = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('score:', rlm.score(X_train, y_train))\n",
    "# print('coefs:', rlm.coef_)\n",
    "# print('intercept:', rlm.intercept_)\n",
    "\n",
    "preds = rlm.predict(X_test)\n",
    "print('rmse:', np.sqrt(mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.9733368524896017\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge().fit(X_train, y_train)\n",
    "\n",
    "preds = ridge.predict(X_test)\n",
    "print('rmse:', np.sqrt(mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 1.977699213240759\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso().fit(X_train, y_train)\n",
    "\n",
    "preds = lasso.predict(X_test)\n",
    "print('rmse:', np.sqrt(mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeCV(cv=10).fit(X_total, y_total)\n",
    "\n",
    "preds = ridge.predict(X_test)\n",
    "print('rmse:', np.sqrt(mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = \n",
    "(cv=30).fit(X_total, y_total)\n",
    "\n",
    "preds = lasso.predict(X_test)\n",
    "print('rmse:', np.sqrt(mean_squared_error(y_test, preds)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.9676393092639556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.269e+00, tolerance: 1.707e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "model = ElasticNetCV(cv=10)\n",
    "model.fit(X_train,y_train)\n",
    "test_predict=model.predict(X_test)\n",
    "print('rmse:', np.sqrt(mean_squared_error(y_test, test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 1.1371871180906692\n"
     ]
    }
   ],
   "source": [
    "ridge = linear_model.RidgeCV(alphas=[0.001,0.02,0.005,0.05,0.1,1,10,100],cv=10).fit(X_train,y_train)\n",
    "pred_ridge = ridge.predict(X_test)\n",
    "print('rmse:', np.sqrt(mean_squared_error(y_test, pred_ridge)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 39.919751816242865, tolerance: 1.5045221612805098\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 61.81840102396018, tolerance: 1.5481476641786667\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 42.88091260360022, tolerance: 1.5860396148022964\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 47.50958446313712, tolerance: 1.5109769098642962\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 64.53077309917535, tolerance: 1.4813816953528798\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 50.98494234869213, tolerance: 1.548038164756187\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 52.300815925543816, tolerance: 1.5252013843593868\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 59.23597812489476, tolerance: 1.5537299965715465\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 64.65908472088461, tolerance: 1.5860410965725276\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 55.03670884133601, tolerance: 1.5149154033895764\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.9671649402313409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.158e+00, tolerance: 1.707e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "#best model\n",
    "lasso = linear_model.LassoCV(alphas=[0.001,0.02,0.05,0.01,0.05,0.1,1,10,100],cv=10).fit(X_train,y_train)\n",
    "pred_lasso = lasso.predict(X_test)\n",
    "print('rmse:', np.sqrt(mean_squared_error(y_test, pred_lasso)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: conda-script.py [-h] [-V] command ...\n",
      "conda-script.py: error: unrecognized arguments: py-xgboost\n"
     ]
    }
   ],
   "source": [
    "conda install -c anaconda py-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.9.1-py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\caill\\anaconda3\\anaconda\\lib\\site-packages (from imbalanced-learn->imblearn) (1.5.0)\n",
      "Collecting joblib>=1.0.0\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\caill\\anaconda3\\anaconda\\lib\\site-packages (from imbalanced-learn->imblearn) (2.1.0)\n",
      "Collecting scikit-learn>=1.1.0\n",
      "  Downloading scikit_learn-1.1.3-cp38-cp38-win_amd64.whl (7.5 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\caill\\anaconda3\\anaconda\\lib\\site-packages (from imbalanced-learn->imblearn) (1.18.5)\n",
      "Installing collected packages: joblib, scikit-learn, imbalanced-learn, imblearn\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 0.16.0\n",
      "    Uninstalling joblib-0.16.0:\n",
      "      Successfully uninstalled joblib-0.16.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "Successfully installed imbalanced-learn-0.9.1 imblearn-0.0 joblib-1.2.0 scikit-learn-1.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:29:09] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:29:18] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:29:27] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:29:35] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:29:44] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:29:53] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:30:04] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:30:14] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:30:24] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:30:35] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:30:46] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:30:58] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:31:10] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:31:23] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:31:36] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:31:48] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:31:58] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:32:08] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:32:19] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:32:29] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:32:39] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:32:51] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:33:05] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:33:17] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:33:31] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:33:44] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:34:00] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:34:16] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:34:33] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:34:48] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:35:03] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:35:12] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:35:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:35:28] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:35:37] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:35:46] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:35:57] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:36:07] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:36:17] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:36:26] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:36:37] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:36:48] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:37:00] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:37:11] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:37:24] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:37:36] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:37:47] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:37:58] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:38:09] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:38:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:38:30] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:38:42] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:38:54] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:39:06] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:39:19] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:39:32] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:39:48] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:40:03] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:40:19] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:40:35] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:40:51] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "The best hyperparameters are  {'learning_rate': 0.015, 'max_depth': 4, 'n_estimators': 700}\n"
     ]
    }
   ],
   "source": [
    "import pandas  as pd\n",
    "import numpy   as np\n",
    "import xgboost as xgb\n",
    "#=========================================================================\n",
    "# XGBoost regression: \n",
    "# Parameters: \n",
    "# n_estimators  \"Number of gradient boosted trees. Equivalent to number \n",
    "#                of boosting rounds.\"\n",
    "# learning_rate \"Boosting learning rate (also known as “eta”)\"\n",
    "# max_depth     \"Maximum depth of a tree. Increasing this value will make \n",
    "#                the model more complex and more likely to overfit.\" \n",
    "#=========================================================================\n",
    "regressor=xgb.XGBRegressor(eval_metric='rmse')\n",
    "\n",
    "#=========================================================================\n",
    "# exhaustively search for the optimal hyperparameters\n",
    "#=========================================================================\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# set up our search grid\n",
    "param_grid = {\"max_depth\":    [4, 5],\n",
    "              \"n_estimators\": [500, 600, 700],\n",
    "              \"learning_rate\": [0.01, 0.015]}\n",
    "\n",
    "# try out every combination of the above values\n",
    "search = GridSearchCV(regressor, param_grid, cv=5).fit(X_train, y_train)\n",
    "\n",
    "print(\"The best hyperparameters are \",search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=xgb.XGBRegressor(learning_rate = search.best_params_[\"learning_rate\"],\n",
    "                           n_estimators  = search.best_params_[\"n_estimators\"],\n",
    "                           max_depth     = search.best_params_[\"max_depth\"],)\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "#=========================================================================\n",
    "# To use early_stopping_rounds: \n",
    "# \"Validation metric needs to improve at least once in every \n",
    "# early_stopping_rounds round(s) to continue training.\"\n",
    "#=========================================================================\n",
    "# first perform a test/train split \n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train,X_test,y_train,y_test = train_test_split(X_train,y_train, test_size = 0.2)\n",
    "#regressor.fit(X_train, y_train, early_stopping_rounds=6, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "#=========================================================================\n",
    "# use the model to predict the prices for the test data\n",
    "#=========================================================================\n",
    "predictions_xgb = regressor.predict(X_test)\n",
    "print('rmse:', np.sqrt(mean_squared_error(y_test, pred_lasso)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X291</th>\n",
       "      <th>X292</th>\n",
       "      <th>X293</th>\n",
       "      <th>X294</th>\n",
       "      <th>X295</th>\n",
       "      <th>X296</th>\n",
       "      <th>X297</th>\n",
       "      <th>X298</th>\n",
       "      <th>X299</th>\n",
       "      <th>X300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5628</td>\n",
       "      <td>-1.1602</td>\n",
       "      <td>-2.9507</td>\n",
       "      <td>1.8596</td>\n",
       "      <td>0.6111</td>\n",
       "      <td>0.5426</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>-1.4185</td>\n",
       "      <td>1.5721</td>\n",
       "      <td>0.3589</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2793</td>\n",
       "      <td>-2.6206</td>\n",
       "      <td>-1.8198</td>\n",
       "      <td>0.3485</td>\n",
       "      <td>-3.7212</td>\n",
       "      <td>-1.3297</td>\n",
       "      <td>0.5444</td>\n",
       "      <td>-2.6967</td>\n",
       "      <td>0.4457</td>\n",
       "      <td>0.2822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.6262</td>\n",
       "      <td>-5.0657</td>\n",
       "      <td>3.8937</td>\n",
       "      <td>-0.8431</td>\n",
       "      <td>1.4812</td>\n",
       "      <td>-5.1453</td>\n",
       "      <td>-3.6389</td>\n",
       "      <td>-2.4859</td>\n",
       "      <td>-0.5881</td>\n",
       "      <td>1.1751</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5679</td>\n",
       "      <td>1.7322</td>\n",
       "      <td>-2.5014</td>\n",
       "      <td>3.2384</td>\n",
       "      <td>4.3442</td>\n",
       "      <td>-1.2154</td>\n",
       "      <td>-0.0337</td>\n",
       "      <td>1.7523</td>\n",
       "      <td>1.9810</td>\n",
       "      <td>1.1596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.4231</td>\n",
       "      <td>-0.6759</td>\n",
       "      <td>3.4278</td>\n",
       "      <td>-1.9188</td>\n",
       "      <td>-2.9790</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>3.3254</td>\n",
       "      <td>-3.6777</td>\n",
       "      <td>0.2251</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3510</td>\n",
       "      <td>-0.9254</td>\n",
       "      <td>2.1161</td>\n",
       "      <td>3.1807</td>\n",
       "      <td>1.2680</td>\n",
       "      <td>-0.6121</td>\n",
       "      <td>-2.8853</td>\n",
       "      <td>0.6135</td>\n",
       "      <td>-3.8482</td>\n",
       "      <td>-2.6199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.2450</td>\n",
       "      <td>-2.4621</td>\n",
       "      <td>-0.2132</td>\n",
       "      <td>-1.5154</td>\n",
       "      <td>0.7104</td>\n",
       "      <td>-0.2637</td>\n",
       "      <td>1.6482</td>\n",
       "      <td>-3.9033</td>\n",
       "      <td>-2.9586</td>\n",
       "      <td>0.1263</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.7182</td>\n",
       "      <td>-3.7176</td>\n",
       "      <td>1.3822</td>\n",
       "      <td>-1.3763</td>\n",
       "      <td>2.4360</td>\n",
       "      <td>2.2482</td>\n",
       "      <td>-0.1899</td>\n",
       "      <td>-2.1956</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>0.3242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.3664</td>\n",
       "      <td>-1.6974</td>\n",
       "      <td>-3.1624</td>\n",
       "      <td>-2.9562</td>\n",
       "      <td>-4.0695</td>\n",
       "      <td>0.7646</td>\n",
       "      <td>2.1250</td>\n",
       "      <td>0.1767</td>\n",
       "      <td>0.8376</td>\n",
       "      <td>3.4000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1817</td>\n",
       "      <td>-4.2374</td>\n",
       "      <td>-0.7468</td>\n",
       "      <td>0.2074</td>\n",
       "      <td>-1.7845</td>\n",
       "      <td>0.4862</td>\n",
       "      <td>-0.1219</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>-0.0297</td>\n",
       "      <td>-3.3259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-2.4636</td>\n",
       "      <td>0.0861</td>\n",
       "      <td>-0.0553</td>\n",
       "      <td>-2.7397</td>\n",
       "      <td>-3.0771</td>\n",
       "      <td>0.7528</td>\n",
       "      <td>-2.5472</td>\n",
       "      <td>-2.8367</td>\n",
       "      <td>-0.4031</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.6057</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>-0.2767</td>\n",
       "      <td>-2.7545</td>\n",
       "      <td>0.4721</td>\n",
       "      <td>-1.1899</td>\n",
       "      <td>2.9520</td>\n",
       "      <td>0.3024</td>\n",
       "      <td>-4.1785</td>\n",
       "      <td>1.1802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>-2.5863</td>\n",
       "      <td>-1.0198</td>\n",
       "      <td>2.1987</td>\n",
       "      <td>-0.1906</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>-1.6413</td>\n",
       "      <td>0.1758</td>\n",
       "      <td>-0.2449</td>\n",
       "      <td>-1.6953</td>\n",
       "      <td>0.6819</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0914</td>\n",
       "      <td>2.4788</td>\n",
       "      <td>1.9824</td>\n",
       "      <td>-1.4055</td>\n",
       "      <td>0.5563</td>\n",
       "      <td>-0.2967</td>\n",
       "      <td>2.1128</td>\n",
       "      <td>2.2533</td>\n",
       "      <td>-0.6875</td>\n",
       "      <td>-0.3318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1.3175</td>\n",
       "      <td>-2.3258</td>\n",
       "      <td>0.3156</td>\n",
       "      <td>0.4093</td>\n",
       "      <td>0.5979</td>\n",
       "      <td>-2.4182</td>\n",
       "      <td>2.0316</td>\n",
       "      <td>-3.2985</td>\n",
       "      <td>-0.8987</td>\n",
       "      <td>0.3122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0679</td>\n",
       "      <td>-1.3411</td>\n",
       "      <td>-1.2035</td>\n",
       "      <td>-1.8944</td>\n",
       "      <td>0.5637</td>\n",
       "      <td>-3.4122</td>\n",
       "      <td>-1.4875</td>\n",
       "      <td>-1.0269</td>\n",
       "      <td>0.6859</td>\n",
       "      <td>2.0488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-0.4030</td>\n",
       "      <td>1.3336</td>\n",
       "      <td>-1.5292</td>\n",
       "      <td>-1.1604</td>\n",
       "      <td>-0.4998</td>\n",
       "      <td>-1.2021</td>\n",
       "      <td>3.5747</td>\n",
       "      <td>1.3613</td>\n",
       "      <td>-0.4823</td>\n",
       "      <td>2.6302</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.6402</td>\n",
       "      <td>1.1191</td>\n",
       "      <td>-0.0174</td>\n",
       "      <td>-0.1273</td>\n",
       "      <td>-1.4689</td>\n",
       "      <td>-2.1417</td>\n",
       "      <td>-2.9054</td>\n",
       "      <td>-3.0942</td>\n",
       "      <td>0.0334</td>\n",
       "      <td>0.7268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.6881</td>\n",
       "      <td>-0.0539</td>\n",
       "      <td>-0.2508</td>\n",
       "      <td>3.7884</td>\n",
       "      <td>3.1900</td>\n",
       "      <td>-2.0129</td>\n",
       "      <td>2.9978</td>\n",
       "      <td>3.5404</td>\n",
       "      <td>1.0865</td>\n",
       "      <td>-0.0622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6429</td>\n",
       "      <td>-1.2308</td>\n",
       "      <td>-2.6267</td>\n",
       "      <td>-0.8491</td>\n",
       "      <td>-2.4038</td>\n",
       "      <td>1.3639</td>\n",
       "      <td>-2.5999</td>\n",
       "      <td>-1.6303</td>\n",
       "      <td>3.5642</td>\n",
       "      <td>2.4358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1      X2      X3      X4      X5      X6      X7      X8      X9  \\\n",
       "0    0.5628 -1.1602 -2.9507  1.8596  0.6111  0.5426  0.8125 -1.4185  1.5721   \n",
       "1    4.6262 -5.0657  3.8937 -0.8431  1.4812 -5.1453 -3.6389 -2.4859 -0.5881   \n",
       "2   -3.4231 -0.6759  3.4278 -1.9188 -2.9790  0.1177  3.3254 -3.6777  0.2251   \n",
       "3    2.2450 -2.4621 -0.2132 -1.5154  0.7104 -0.2637  1.6482 -3.9033 -2.9586   \n",
       "4    1.3664 -1.6974 -3.1624 -2.9562 -4.0695  0.7646  2.1250  0.1767  0.8376   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "195 -2.4636  0.0861 -0.0553 -2.7397 -3.0771  0.7528 -2.5472 -2.8367 -0.4031   \n",
       "196 -2.5863 -1.0198  2.1987 -0.1906  0.0557 -1.6413  0.1758 -0.2449 -1.6953   \n",
       "197  1.3175 -2.3258  0.3156  0.4093  0.5979 -2.4182  2.0316 -3.2985 -0.8987   \n",
       "198 -0.4030  1.3336 -1.5292 -1.1604 -0.4998 -1.2021  3.5747  1.3613 -0.4823   \n",
       "199  0.6881 -0.0539 -0.2508  3.7884  3.1900 -2.0129  2.9978  3.5404  1.0865   \n",
       "\n",
       "        X10  ...    X291    X292    X293    X294    X295    X296    X297  \\\n",
       "0    0.3589  ... -0.2793 -2.6206 -1.8198  0.3485 -3.7212 -1.3297  0.5444   \n",
       "1    1.1751  ... -0.5679  1.7322 -2.5014  3.2384  4.3442 -1.2154 -0.0337   \n",
       "2    0.7657  ... -0.3510 -0.9254  2.1161  3.1807  1.2680 -0.6121 -2.8853   \n",
       "3    0.1263  ... -2.7182 -3.7176  1.3822 -1.3763  2.4360  2.2482 -0.1899   \n",
       "4    3.4000  ... -0.1817 -4.2374 -0.7468  0.2074 -1.7845  0.4862 -0.1219   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "195  0.3337  ... -1.6057  0.5656 -0.2767 -2.7545  0.4721 -1.1899  2.9520   \n",
       "196  0.6819  ...  1.0914  2.4788  1.9824 -1.4055  0.5563 -0.2967  2.1128   \n",
       "197  0.3122  ... -0.0679 -1.3411 -1.2035 -1.8944  0.5637 -3.4122 -1.4875   \n",
       "198  2.6302  ... -1.6402  1.1191 -0.0174 -0.1273 -1.4689 -2.1417 -2.9054   \n",
       "199 -0.0622  ...  0.6429 -1.2308 -2.6267 -0.8491 -2.4038  1.3639 -2.5999   \n",
       "\n",
       "       X298    X299    X300  \n",
       "0   -2.6967  0.4457  0.2822  \n",
       "1    1.7523  1.9810  1.1596  \n",
       "2    0.6135 -3.8482 -2.6199  \n",
       "3   -2.1956  0.4329  0.3242  \n",
       "4    0.5714 -0.0297 -3.3259  \n",
       "..      ...     ...     ...  \n",
       "195  0.3024 -4.1785  1.1802  \n",
       "196  2.2533 -0.6875 -0.3318  \n",
       "197 -1.0269  0.6859  2.0488  \n",
       "198 -3.0942  0.0334  0.7268  \n",
       "199 -1.6303  3.5642  2.4358  \n",
       "\n",
       "[200 rows x 300 columns]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 52.51378911189863, tolerance: 2.0371182652179955\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31.86697233043003, tolerance: 2.0614505438687822\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 69.80205054640714, tolerance: 2.047742865772279\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 44.541764051971654, tolerance: 2.0209668910501826\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 44.55581505974635, tolerance: 2.0805035361519324\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.118615996270364, tolerance: 2.05387286734864\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 40.324726020982666, tolerance: 2.030266799102146\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 40.2567864382313, tolerance: 2.0730270000222224\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 49.191561323122414, tolerance: 1.990749907737\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 41.30517315065322, tolerance: 2.0173278860569104\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\caill\\anaconda3\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.477e+01, tolerance: 2.268e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "#Entrainement du modèle final\n",
    "df = pd.read_csv('data_challenge.txt', sep=' ')\n",
    "X_total = df.drop(columns='y')\n",
    "y_total = df.y\n",
    "df_final = pd.read_csv('dataX_test.txt', sep=' ')\n",
    "lasso = linear_model.LassoCV(alphas=[0.001,0.02,0.05,0.01,0.05,0.1,1,10,100],cv=10).fit(X_total,y_total)\n",
    "pred_lasso = lasso.predict(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultat = pred_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.82442108446711\n",
      "-8.957313378024073\n",
      "-8.558034067712\n",
      "-4.86117584075062\n",
      "-9.77208852087284\n",
      "-3.030639690503622\n",
      "-3.763350435491221\n",
      "2.2750507481311075\n",
      "5.9335521617722975\n",
      "1.961282131234083\n",
      "3.320906622328437\n",
      "2.244647141678322\n",
      "-3.8182357536721456\n",
      "-5.295172860015077\n",
      "-5.155385340118813\n",
      "2.8678090792343234\n",
      "-7.252079233960557\n",
      "2.639248670445114\n",
      "-5.7450144896389475\n",
      "-3.1376606265393567\n",
      "-3.309301643138085\n",
      "4.455604478296112\n",
      "1.8485243678940853\n",
      "-7.6126960696044215\n",
      "1.4771012556156156\n",
      "8.402658341339734\n",
      "-4.34249525905725\n",
      "-1.5607965332429639\n",
      "-2.9551499766462466\n",
      "6.188516024218411\n",
      "-3.8181679524727246\n",
      "6.26495120051264\n",
      "-10.823329807856872\n",
      "-6.949427912739719\n",
      "5.600206160016737\n",
      "2.331726369882034\n",
      "0.7974216151898315\n",
      "-1.8860179335996494\n",
      "-12.987987631724762\n",
      "-2.417974607520096\n",
      "13.35639369335454\n",
      "-8.758908639196896\n",
      "-0.7737179208191297\n",
      "10.78843674331821\n",
      "-10.92454258533392\n",
      "-8.208017747684531\n",
      "4.153647698037746\n",
      "-0.28918629774871074\n",
      "-1.7112080194880899\n",
      "-1.3103210767071305\n",
      "0.527994952713781\n",
      "9.424817138618407\n",
      "5.097458053861608\n",
      "-0.395662050584228\n",
      "-1.5296265865359295\n",
      "7.953209961553159\n",
      "13.097563545508025\n",
      "7.95521463780605\n",
      "-2.4860764711379844\n",
      "-0.1696065892046632\n",
      "2.3148707749275297\n",
      "-1.351263954400844\n",
      "11.288517523263188\n",
      "1.2875698904985082\n",
      "3.516999228770988\n",
      "-7.208721726326202\n",
      "-6.74411895583841\n",
      "1.611387241944493\n",
      "-0.37474326898351384\n",
      "-6.301830579868274\n",
      "-3.0017908023495847\n",
      "3.9484780069136614\n",
      "-7.247021525900388\n",
      "5.397061313195101\n",
      "0.6136207662075456\n",
      "-4.799481886131428\n",
      "1.759796540291195\n",
      "-8.815854157425795\n",
      "3.6231857789638955\n",
      "0.19660153000889155\n",
      "-0.12848698176851853\n",
      "-2.970103631503278\n",
      "0.5239487532876707\n",
      "-1.387116736521437\n",
      "2.3915196008177464\n",
      "2.1770268834428883\n",
      "-7.8104556046685465\n",
      "-6.2971924039910485\n",
      "-0.9571665868228153\n",
      "4.203154080512112\n",
      "0.7652769751501161\n",
      "-10.490744049923274\n",
      "-3.27535731402586\n",
      "4.00245164664505\n",
      "-3.810794818219037\n",
      "-0.7441480613296596\n",
      "7.990216599295926\n",
      "1.9165334836396308\n",
      "3.2039527711258344\n",
      "3.080498882698995\n",
      "7.798007834704142\n",
      "2.1017593798231955\n",
      "2.1338396804957016\n",
      "2.3337572678500065\n",
      "2.1999894238998854\n",
      "6.272999200892564\n",
      "-3.2197387394701185\n",
      "5.9736349284503545\n",
      "-7.902038091083055\n",
      "1.3177707620195855\n",
      "-2.2593595970711235\n",
      "3.5815073401829767\n",
      "-8.244442002479529\n",
      "4.525284181899143\n",
      "3.7771010790435118\n",
      "7.016204717609045\n",
      "0.543412339162904\n",
      "-0.8583213600578216\n",
      "-0.06437776856341001\n",
      "-3.122390238524344\n",
      "0.8454516888205568\n",
      "16.38793673996843\n",
      "1.6922704830399442\n",
      "3.468645690892374\n",
      "-1.6144855670003486\n",
      "-0.3525767287191074\n",
      "10.239949861365123\n",
      "3.4455471786455725\n",
      "-0.9566607283997849\n",
      "4.0280639226868\n",
      "9.669327166047823\n",
      "0.5299776639253132\n",
      "0.6194876906175711\n",
      "-3.3484722238554427\n",
      "0.4250648241709045\n",
      "5.565626423542695\n",
      "-1.1166522667181775\n",
      "-5.951060556246556\n",
      "6.14732278069912\n",
      "-5.606050373202728\n",
      "1.9339619196334934\n",
      "-0.15885437227136423\n",
      "-1.2016078969316215\n",
      "-0.407492101720796\n",
      "0.7183424478346648\n",
      "-5.545615512718054\n",
      "3.0040458846888267\n",
      "14.66559759352078\n",
      "-7.133703087863696\n",
      "-0.06048123390289817\n",
      "-0.3518402924124977\n",
      "-0.6351705454997925\n",
      "-0.868317590349652\n",
      "1.2954016207452907\n",
      "0.08556704367089335\n",
      "-12.78790111826511\n",
      "-2.956879528466973\n",
      "2.465662193468417\n",
      "3.3816633657554434\n",
      "-2.188273336114535\n",
      "0.29402507434803415\n",
      "-5.527946367926664\n",
      "2.349875484215323\n",
      "4.371934556407656\n",
      "-8.681365718642434\n",
      "-3.6167307561484576\n",
      "-7.586465264407509\n",
      "4.822030745674254\n",
      "-7.134876713606999\n",
      "8.550681778562756\n",
      "2.942660555793003\n",
      "-3.8752035658669515\n",
      "-3.1714372157037367\n",
      "4.071255929233861\n",
      "0.9613159258864009\n",
      "3.9212810666362343\n",
      "3.313020195349744\n",
      "-3.53619964910389\n",
      "2.330072959151293\n",
      "1.4738026658561911\n",
      "-2.796247748926722\n",
      "-5.037056916197968\n",
      "-3.7869994982144823\n",
      "-2.4787033865756\n",
      "0.6406493495935007\n",
      "4.78008880522547\n",
      "2.3904480698853163\n",
      "10.175453924350574\n",
      "-5.342138739161372\n",
      "-6.708273248150751\n",
      "1.939276995120994\n",
      "1.4092272307799236\n",
      "-3.5898342308878277\n",
      "-6.293433635229919\n",
      "4.428424662833386\n",
      "-2.348634261633289\n",
      "-5.13396380297777\n",
      "2.5837519654868344\n",
      "4.173572369550816\n",
      "4.487977143958847\n"
     ]
    }
   ],
   "source": [
    "for i in resultat:\n",
    "    print(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
